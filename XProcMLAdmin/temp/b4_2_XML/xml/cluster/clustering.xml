<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="clustering.css" type="text/css" charset="UTF-8"?>
<XML>
<TITLE> </TITLE><Heading-1>
<A ID="pgfId-1060644"></A>
<A ID="12050"></A>
Clustering in MarkLogic Server</Heading-1>
<pagenum>
<A ID="pgfId-1060648"></A>
25</pagenum>
<Body>
<A ID="pgfId-1058747"></A>
This chapter describes the basics of how clustering works in MarkLogic Server, and includes the following sections:</Body>
<Body-bullet>
<A ID="pgfId-1060606"></A>
<A href="clustering.xml#id(36521)" xml:link="simple" show="replace" actuate="user" CLASS="XRef"><Hyperlink>
Overview of Clustering</Hyperlink>
</A></Body-bullet>
<Body-bullet>
<A ID="pgfId-1060930"></A>
<A href="clustering.xml#id(32617)" xml:link="simple" show="replace" actuate="user" CLASS="XRef"><Hyperlink>
Evaluator/Data Node Architecture</Hyperlink>
</A></Body-bullet>
<Body-bullet>
<A ID="pgfId-1060614"></A>
<A href="clustering.xml#id(87995)" xml:link="simple" show="replace" actuate="user" CLASS="XRef"><Hyperlink>
Communication Between Nodes</Hyperlink>
</A></Body-bullet>
<Body-bullet>
<A ID="pgfId-1060917"></A>
<A href="clustering.xml#id(38365)" xml:link="simple" show="replace" actuate="user" CLASS="XRef"><Hyperlink>
Installation</Hyperlink>
</A></Body-bullet>
<Heading-2>
<A ID="pgfId-1060772"></A>
<A ID="36521"></A>
Overview of Clustering</Heading-2>
<Body>
<A ID="pgfId-1060776"></A>
You can combine multiple instances of MarkLogic Server to run as a <Emphasis>
cluster</Emphasis>
. The cluster has multiple machines (<Emphasis>
hosts</Emphasis>
), each running an instance of MarkLogic Server. Each host in a cluster is sometimes called a <Emphasis>
node</Emphasis>
, and each node in the cluster has its own copy of all of the configuration information for the entire cluster. </Body>
<Body>
<A ID="pgfId-1060940"></A>
When deployed as a cluster, MarkLogic Server implements a <Emphasis>
shared-nothing</Emphasis>
 architecture. There is no single host in charge; each host communicates with every other host, and each node in the cluster maintains its own copy of the configuration. The security database, as well as all of the other databases in the cluster, are available to each node in the cluster. This shared-nothing architecture has great advantages when it comes to scalability and availability. As your scalability needs grow, you simply add more nodes. </Body>
<Body>
<A ID="pgfId-1060782"></A>
For more details of how clustering works in MarkLogic Server, see <A href="distributed.xml#id(13921)" xml:link="simple" show="replace" actuate="user" CLASS="XRef">'Getting Started with Enterprise Edition Distributed Deployments' on page 13</A>.</Body>
<Heading-2>
<A ID="pgfId-1060893"></A>
<A ID="32617"></A>
Evaluator/Data Node Architecture</Heading-2>
<Body>
<A ID="pgfId-1060897"></A>
There are two roles a node in a MarkLogic Server cluster can perform:</Body>
<Body-bullet>
<A ID="pgfId-1060898"></A>
Evaluator node (e-node)</Body-bullet>
<Body-bullet>
<A ID="pgfId-1060899"></A>
Data node (d-node)</Body-bullet>
<Body>
<A ID="pgfId-1060900"></A>
E-nodes evaluate XQuery programs, XCC/XDBC requests, WebDAV requests, and other server requests. If the request does not need any forest data to complete, then an e-node request is evaluated entirely on the e-node. If the request needs forest data (for example, a document in a database), then it communicates with one or more d-nodes to service the forest data. Once it gets the content back from the d-node, the e-node finishes processing the request (performs the filter portion of query processing) and sends the results to the application.</Body>
<Body>
<A ID="pgfId-1060901"></A>
D-nodes are responsible for maintaining transactional integrity during insert, update, and delete operations. This transactional integrity includes forest journaling, forest recovery, backup operations, and on-disk forest management. D-nodes are also responsible for providing forest optimization (merges), index maintenance, and content retrieval. D-nodes service e-nodes when the e-nodes require content returned from a forest. A d-node gets the communication from an e-node, then sends the results of the index resolution back to the e-node. The d-node part of the request includes the index resolution portion of query processing. Also, each d-node performs the work needed for merges for any forests hosted on that node.</Body>
<Body>
<A ID="pgfId-1060902"></A>
It is possible for a single node to act as both an e-node and a d-node. In single host configurations, both e-node and d-node activities are carried out by a single host. In a cluster, it is also possible for some or all of the hosts to have shared e-node and d-node duties. In large configurations, however, it is usually best have e-nodes and d-nodes operate on separate hosts in the cluster. For more details about this distributed architecture, see <A href="distributed.xml#id(13921)" xml:link="simple" show="replace" actuate="user" CLASS="XRef">'Getting Started with Enterprise Edition Distributed Deployments' on page 13</A>.</Body>
<Heading-2>
<A ID="pgfId-1060467"></A>
<A ID="87995"></A>
Communication Between Nodes</Heading-2>
<Body>
<A ID="pgfId-1060468"></A>
Each node in a cluster communicates with all of the other nodes in the cluster at periodic intervals. This periodic communication, known as a <Emphasis>
heartbeat</Emphasis>
, circulates key information about host status and availability between the nodes in a cluster. Through this mechanism, the cluster determines which nodes are available and communicates configuration changes with other nodes in the cluster. If a node goes down for some reason, it will stop sending heartbeats to the other nodes in the cluster. </Body>
<Body>
<A ID="pgfId-1060984"></A>
The cluster uses the heartbeat to determine if a node in the cluster is down. A heartbeat from a given node communicates its view of the cluster at the moment of the heartbeat. This determination is based on a vote from each node in the cluster, based on each node's view of the current state of the cluster. In order to vote a node out of the cluster, there must be a <Emphasis>
quorum</Emphasis>
 of nodes voting to remove a node. A quorum occurs if 50% or more of the <Emphasis>
total</Emphasis>
 number of nodes in the cluster (including any nodes that are down) vote the same way. The voting that each host performs is done based on how long it has been since it last had a heartbeat from the other node. If at least half of the nodes in the cluster determine that a node is down, then that node is disconnected from the cluster. The wait time for a host to be disconnected from the cluster is typically considerably longer than the time for restarting a host, so restarts should not cause hosts to be disconnected from the cluster (and therefore they should not cause forests to fail over). There are configuration parameters to determine how long to wait before removing a node (for details, see <A href="config-both-failover.xml#id(95009)" xml:link="simple" show="replace" actuate="user" CLASS="XRef">'XDQP Timeout, Host Timeout, and Host Initial Timeout Parameters' on page 51</A>).</Body>
<Body>
<A ID="pgfId-1061002"></A>
Each node in the cluster continues listening for the heartbeat from the disconnected node to see if it has come back up, and if a quorum of nodes in the cluster are getting heartbeats from the node, then it automatically rejoins the cluster.</Body>
<Body>
<A ID="pgfId-1060844"></A>
The heartbeat mechanism allows the cluster to recover gracefully from things like hardware failures or other events that might make a host unresponsive. This occurs automatically, without any human intervention; machines can go down and automatically come back up without requiring intervention from an administrator. If the node that goes down hosts content in a forest, then the database to which that forest belongs will go offline until the forest either comes back up or is detached from the database. If you have failover enabled and configured for that forest, it will attempt to fail over the forest to a secondary host (that is, one of the secondary hosts will attempt to mount the forest). Once that occurs, the database will come back online. For details on failover, see <A href="failover.xml#id(82157)" xml:link="simple" show="replace" actuate="user" CLASS="XRef">'High Availability of Data Nodes With Failover' on page 26</A>.</Body>
<Heading-2>
<A ID="pgfId-1060873"></A>
<A ID="38365"></A>
Installation</Heading-2>
<Body>
<A ID="pgfId-1060877"></A>
Installation of new nodes in a cluster is simply a matter of installing MarkLogic Server on a machine and entering the connection information for any existing node in the cluster you want to join. Once a node joins a cluster, depending on how that node is configured, you can use it to process queries and/or to manage content in forests. Under normal conditions, you can also perform cluster-wide administrative tasks from any host in the cluster. For details on the installation process, see the <Emphasis>
Installation Guide</Emphasis>
.</Body>
</XML>
